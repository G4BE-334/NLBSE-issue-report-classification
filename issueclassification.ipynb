{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLBSE Challenge 2024: Issue Report Classification\n",
    "\n",
    "See more at https://nlbse2024.github.io/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-08-26 06:33:37</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot add node \"1\" because a n...</td>\n",
       "      <td>### Website or app\\n\\nPrivate repo cannot give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-28 05:16:12</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Devtools extension build faili...</td>\n",
       "      <td>### Website or app\\n\\nN/A\\n\\n### Repro steps\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-13 21:58:31</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...</td>\n",
       "      <td>### Website or app\\n\\nhttps://github.com/open-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-06-14 02:31:20</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot remove node \"0\" because ...</td>\n",
       "      <td>### Website or app\\n\\nlocal\\n\\n### Repro steps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-06-03 11:29:44</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot remove node \"103\" becaus...</td>\n",
       "      <td>### Website or app\\n\\nlocalhost\\n\\n### Repro s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-24 10:48:13</td>\n",
       "      <td>feature</td>\n",
       "      <td>core: FP denormals support</td>\n",
       "      <td>relates #21046\\r\\n\\r\\n- support x86 SSE FTZ+DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-20 12:40:55</td>\n",
       "      <td>feature</td>\n",
       "      <td>feature: submodule or a class scope for export...</td>\n",
       "      <td>All classes are registered in the scope that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-15 02:39:22</td>\n",
       "      <td>feature</td>\n",
       "      <td>Reading BigTiff images</td>\n",
       "      <td>**Merge with extra: https://github.com/opencv/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-14 15:37:53</td>\n",
       "      <td>feature</td>\n",
       "      <td>Add general broadcasting layer</td>\n",
       "      <td>Performance details(broadcasting 1x1 to 16x204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-11 16:30:53</td>\n",
       "      <td>feature</td>\n",
       "      <td>Adapt remote inference to operate with NV12 blobs</td>\n",
       "      <td>### Pull Request Readiness Checklist\\r\\n\\r\\nSe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                repo           created_at    label  \\\n",
       "0     facebook/react  2023-08-26 06:33:37      bug   \n",
       "1     facebook/react  2023-07-28 05:16:12      bug   \n",
       "2     facebook/react  2023-07-13 21:58:31      bug   \n",
       "3     facebook/react  2023-06-14 02:31:20      bug   \n",
       "4     facebook/react  2023-06-03 11:29:44      bug   \n",
       "...              ...                  ...      ...   \n",
       "1495   opencv/opencv  2022-01-24 10:48:13  feature   \n",
       "1496   opencv/opencv  2022-01-20 12:40:55  feature   \n",
       "1497   opencv/opencv  2022-01-15 02:39:22  feature   \n",
       "1498   opencv/opencv  2022-01-14 15:37:53  feature   \n",
       "1499   opencv/opencv  2022-01-11 16:30:53  feature   \n",
       "\n",
       "                                                  title  \\\n",
       "0     [DevTools Bug] Cannot add node \"1\" because a n...   \n",
       "1     [DevTools Bug]: Devtools extension build faili...   \n",
       "2     [DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...   \n",
       "3     [DevTools Bug] Cannot remove node \"0\" because ...   \n",
       "4     [DevTools Bug] Cannot remove node \"103\" becaus...   \n",
       "...                                                 ...   \n",
       "1495                         core: FP denormals support   \n",
       "1496  feature: submodule or a class scope for export...   \n",
       "1497                             Reading BigTiff images   \n",
       "1498                     Add general broadcasting layer   \n",
       "1499  Adapt remote inference to operate with NV12 blobs   \n",
       "\n",
       "                                                   body  \n",
       "0     ### Website or app\\n\\nPrivate repo cannot give...  \n",
       "1     ### Website or app\\n\\nN/A\\n\\n### Repro steps\\n...  \n",
       "2     ### Website or app\\n\\nhttps://github.com/open-...  \n",
       "3     ### Website or app\\n\\nlocal\\n\\n### Repro steps...  \n",
       "4     ### Website or app\\n\\nlocalhost\\n\\n### Repro s...  \n",
       "...                                                 ...  \n",
       "1495  relates #21046\\r\\n\\r\\n- support x86 SSE FTZ+DA...  \n",
       "1496  All classes are registered in the scope that c...  \n",
       "1497  **Merge with extra: https://github.com/opencv/...  \n",
       "1498  Performance details(broadcasting 1x1 to 16x204...  \n",
       "1499  ### Pull Request Readiness Checklist\\r\\n\\r\\nSe...  \n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/issues_train.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidVectorizer model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.77      0.73      0.75       500\n",
      "     feature       0.75      0.78      0.76       500\n",
      "    question       0.68      0.68      0.68       498\n",
      "\n",
      "    accuracy                           0.73      1498\n",
      "   macro avg       0.73      0.73      0.73      1498\n",
      "weighted avg       0.73      0.73      0.73      1498\n",
      "\n",
      "Accuracy: 73.03%\n",
      "F1-Score (weighted): 73.02%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv') \n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "X_train = train_data[['body', 'title']]\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = test_data[['body', 'title']]\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Create a column transformer that applies TfidfVectorizer to 'body' and 'title' columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', TfidfVectorizer(stop_words='english'), 'body'),\n",
    "        ('title', TfidfVectorizer(stop_words='english'), 'title')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create a pipeline that first applies the column transformer and then trains a classifier\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.73      0.64      0.69       500\n",
      "     feature       0.65      0.78      0.70       500\n",
      "    question       0.65      0.60      0.63       498\n",
      "\n",
      "    accuracy                           0.67      1498\n",
      "   macro avg       0.68      0.67      0.67      1498\n",
      "weighted avg       0.68      0.67      0.67      1498\n",
      "\n",
      "Accuracy: 67.42%\n",
      "F1-Score (weighted): 67.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "# Create a concatenated column of 'title' and 'body'\n",
    "train_data['text'] = train_data['title'] + \" \" + train_data['body']\n",
    "test_data['text'] = test_data['title'] + \" \" + test_data['body']\n",
    "\n",
    "# Tokenize the concatenated text\n",
    "train_data['text_tokenized'] = train_data['text'].apply(lambda x: x.split())\n",
    "test_data['text_tokenized'] = test_data['text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(pd.concat([train_data['text_tokenized'], test_data['text_tokenized']]), vector_size=100, window=5, min_count=2)\n",
    "\n",
    "# Get the embeddings\n",
    "train_data['text_vector'] = train_data['text_tokenized'].apply(lambda x: document_vector(w2v_model, x))\n",
    "test_data['text_vector'] = test_data['text_tokenized'].apply(lambda x: document_vector(w2v_model, x))\n",
    "\n",
    "# Prepare data for training and testing\n",
    "X_train = np.array(list(train_data['text_vector']))\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = np.array(list(test_data['text_vector']))\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Train a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Concatenated Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.74      0.69      0.71       500\n",
      "     feature       0.69      0.75      0.72       500\n",
      "    question       0.66      0.65      0.66       498\n",
      "\n",
      "    accuracy                           0.70      1498\n",
      "   macro avg       0.70      0.70      0.70      1498\n",
      "weighted avg       0.70      0.70      0.70      1498\n",
      "\n",
      "Accuracy: 69.56%\n",
      "F1-Score (weighted): 69.53%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf8') as f:\n",
    "        for line_no, line in enumerate(f, 1):  # Enumerate will give us the line number\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "            except ValueError as e:\n",
    "                print()\n",
    "    return embeddings\n",
    "\n",
    "def document_vector(glove_embeddings, doc):\n",
    "    # Assuming all embeddings in the GloVe file have the same size\n",
    "    embedding_size = next(iter(glove_embeddings.values())).shape[0]\n",
    "    \n",
    "    vectors = [glove_embeddings.get(word, np.zeros(embedding_size)) for word in doc]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove('data/glove.840B.300d/glove.840B.300d.txt')\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "# Create a concatenated column of 'title' and 'body'\n",
    "train_data['text'] = train_data['title'] + \" \" + train_data['body']\n",
    "test_data['text'] = test_data['title'] + \" \" + test_data['body']\n",
    "\n",
    "# Tokenize the concatenated text\n",
    "train_data['text_tokenized'] = train_data['text'].apply(lambda x: x.split())\n",
    "test_data['text_tokenized'] = test_data['text'].apply(lambda x: x.split())\n",
    "\n",
    "# Get the embeddings using GloVe\n",
    "train_data['text_vector'] = train_data['text_tokenized'].apply(lambda x: document_vector(glove_embeddings, x))\n",
    "test_data['text_vector'] = test_data['text_tokenized'].apply(lambda x: document_vector(glove_embeddings, x))\n",
    "\n",
    "# Prepare data for training and testing\n",
    "X_train = np.array(list(train_data['text_vector']))\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = np.array(list(test_data['text_vector']))\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Train a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Separate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.70      0.71      0.71       500\n",
      "     feature       0.68      0.70      0.69       500\n",
      "    question       0.65      0.62      0.63       498\n",
      "\n",
      "    accuracy                           0.68      1498\n",
      "   macro avg       0.68      0.68      0.68      1498\n",
      "weighted avg       0.68      0.68      0.68      1498\n",
      "\n",
      "Accuracy: 67.76%\n",
      "F1-Score (weighted): 67.71%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf8') as f:\n",
    "        for line_no, line in enumerate(f, 1):  # Enumerate will give us the line number\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "            except ValueError as e:\n",
    "                print()\n",
    "    return embeddings\n",
    "\n",
    "def document_vector(glove_embeddings, doc):\n",
    "    # Assuming all embeddings in the GloVe file have the same size\n",
    "    embedding_size = next(iter(glove_embeddings.values())).shape[0]\n",
    "    \n",
    "    vectors = [glove_embeddings.get(word, np.zeros(embedding_size)) for word in doc]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "class GloVeVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, glove_embeddings):\n",
    "        self.glove_embeddings = glove_embeddings\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([document_vector(self.glove_embeddings, x.split()) for x in X])\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove('data/glove.840B.300d/glove.840B.300d.txt')\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "X_train = train_data[['body', 'title']]\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = test_data[['body', 'title']]\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Create a column transformer that applies GloVeVectorizer to 'body' and 'title' columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', GloVeVectorizer(glove_embeddings), 'body'),\n",
    "        ('title', GloVeVectorizer(glove_embeddings), 'title')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create a pipeline that first applies the column transformer and then trains a classifier\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Text Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.61      0.62      0.61       500\n",
      "     feature       0.57      0.73      0.64       500\n",
      "    question       0.65      0.44      0.52       498\n",
      "\n",
      "    accuracy                           0.60      1498\n",
      "   macro avg       0.61      0.60      0.59      1498\n",
      "weighted avg       0.61      0.60      0.59      1498\n",
      "\n",
      "Accuracy: 59.88%\n",
      "F1-Score (weighted): 59.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "def load_fasttext(fasttext_path):\n",
    "    embeddings = {}\n",
    "    with open(fasttext_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def document_vector(fasttext_embeddings, doc):\n",
    "    # Use embeddings to get vectors for each word in the doc\n",
    "    vectors = [fasttext_embeddings.get(word, np.zeros(300)) for word in doc]\n",
    "    # Compute the mean vector for the entire doc\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_embeddings = load_fasttext('data/wiki-news-300d-1M-subword.vec/wiki-news-300d-1M-subword.vec')\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "# Create a concatenated column of 'title' and 'body'\n",
    "train_data['text'] = train_data['title'] + \" \" + train_data['body']\n",
    "test_data['text'] = test_data['title'] + \" \" + test_data['body']\n",
    "\n",
    "# Tokenize the concatenated text\n",
    "train_data['text_tokenized'] = train_data['text'].apply(lambda x: x.split())\n",
    "test_data['text_tokenized'] = test_data['text'].apply(lambda x: x.split())\n",
    "\n",
    "# Get the embeddings using FastText\n",
    "train_data['text_vector'] = train_data['text_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "test_data['text_vector'] = test_data['text_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "\n",
    "# Prepare data for training and testing\n",
    "X_train = np.array(list(train_data['text_vector']))\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = np.array(list(test_data['text_vector']))\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Train a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Separate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.65      0.72      0.68       500\n",
      "     feature       0.66      0.75      0.70       500\n",
      "    question       0.68      0.51      0.59       498\n",
      "\n",
      "    accuracy                           0.66      1498\n",
      "   macro avg       0.66      0.66      0.66      1498\n",
      "weighted avg       0.66      0.66      0.66      1498\n",
      "\n",
      "Accuracy: 66.09%\n",
      "F1-Score (weighted): 65.65%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "def load_fasttext(fasttext_path):\n",
    "    embeddings = {}\n",
    "    with open(fasttext_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def document_vector(fasttext_embLineags.get(word, np.zeros(300)) for word in doc]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_embeddings = load_fasttext('data/wiki-news-300d-1M-subword.vec/wiki-news-300d-1M-subword.vec')\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "# Tokenize the title and body\n",
    "train_data['title_tokenized'] = train_data['title'].apply(lambda x: x.split())\n",
    "train_data['body_tokenized'] = train_data['body'].apply(lambda x: x.split())\n",
    "\n",
    "test_data['title_tokenized'] = test_data['title'].apply(lambda x: x.split())\n",
    "test_data['body_tokenized'] = test_data['body'].apply(lambda x: x.split())\n",
    "\n",
    "# Get the embeddings for title and body using FastText\n",
    "train_data['title_vector'] = train_data['title_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "train_data['body_vector'] = train_data['body_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "\n",
    "test_data['title_vector'] = test_data['title_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "test_data['body_vector'] = test_data['body_tokenized'].apply(lambda x: document_vector(fasttext_embeddings, x))\n",
    "\n",
    "# Prepare data for training and testing by concatenating title and body vectors for each record\n",
    "X_train = np.hstack((np.array(list(train_data['title_vector'])), np.array(list(train_data['body_vector']))))\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = np.hstack((np.array(list(test_data['title_vector'])), np.array(list(test_data['body_vector']))))\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Train a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.61      0.44      0.51       500\n",
      "     feature       0.62      0.58      0.60       500\n",
      "    question       0.48      0.65      0.55       498\n",
      "\n",
      "    accuracy                           0.56      1498\n",
      "   macro avg       0.57      0.56      0.55      1498\n",
      "weighted avg       0.57      0.56      0.55      1498\n",
      "\n",
      "Accuracy: 55.61%\n",
      "F1-Score (weighted): 55.45%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, epochs=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.d2v_model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tagged_data = [TaggedDocument(words=_d.split(), tags=[str(i)]) for i, _d in enumerate(X)]\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers, epochs=self.epochs)\n",
    "        self.d2v_model.build_vocab(tagged_data)\n",
    "        self.d2v_model.train(tagged_data, total_examples=self.d2v_model.corpus_count, epochs=self.d2v_model.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.d2v_model.infer_vector(_d.split()) for _d in X])\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "test_data = pd.read_csv('./data/issues_test.csv')\n",
    "\n",
    "# Drop rows where 'title' or 'body' is NaN\n",
    "train_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "test_data.dropna(subset=['title', 'body'], inplace=True)\n",
    "\n",
    "X_train = train_data[['body', 'title']]\n",
    "y_train = train_data['label']\n",
    "X_test = test_data[['body', 'title']]\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Create a column transformer that applies Doc2VecTransformer to 'body' and 'title' columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', Doc2VecTransformer(), 'body'),\n",
    "        ('title', Doc2VecTransformer(), 'title')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Use a RandomForest classifier for the pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(f\"F1-Score (weighted): {f1_score(y_test, y_pred, average='weighted')*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 32 is smaller than n_iter=50. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Performance of Logistic Regression after Randomized Search\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.74      0.62      0.68       500\n",
      "     feature       0.65      0.79      0.72       500\n",
      "    question       0.63      0.60      0.62       498\n",
      "\n",
      "    accuracy                           0.67      1498\n",
      "   macro avg       0.68      0.67      0.67      1498\n",
      "weighted avg       0.68      0.67      0.67      1498\n",
      "\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 32 is smaller than n_iter=50. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of LinearSVC after Randomized Search\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.59      0.78      0.67       500\n",
      "     feature       0.66      0.73      0.69       500\n",
      "    question       0.73      0.41      0.52       498\n",
      "\n",
      "    accuracy                           0.64      1498\n",
      "   macro avg       0.66      0.64      0.63      1498\n",
      "weighted avg       0.66      0.64      0.63      1498\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Performance of RandomForest after Randomized Search\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bug       0.67      0.56      0.61       500\n",
      "     feature       0.60      0.66      0.63       500\n",
      "    question       0.52      0.55      0.53       498\n",
      "\n",
      "    accuracy                           0.59      1498\n",
      "   macro avg       0.59      0.59      0.59      1498\n",
      "weighted avg       0.59      0.59      0.59      1498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.w2v = None\n",
    "        self.mean_vector = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Tokenize the text\n",
    "        sentences = [row.split() for row in X]\n",
    "        # Train the Word2Vec model\n",
    "        self.w2v = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
    "        # Pre-compute the mean vectors for the training set\n",
    "        self.mean_vector = np.array([\n",
    "            np.mean([self.w2v.wv[word] for word in words if word in self.w2v.wv] or [np.zeros(self.vector_size)], axis=0)\n",
    "            for words in sentences\n",
    "        ])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Compute the mean vector for the input data\n",
    "        return np.array([\n",
    "            np.mean([self.w2v.wv[word] for word in words.split() if word in self.w2v.wv] or [np.zeros(self.vector_size)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "# The clean_text function and data loading code remain the same\n",
    "\n",
    "# Let's test with a range of models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "param_distributions = {\n",
    "    \"Logistic Regression\": {\n",
    "        'clf__C': [0.1, 1, 10, 100],\n",
    "        'w2v__vector_size': [100, 200],\n",
    "        'w2v__window': [5, 10],\n",
    "        'w2v__min_count': [1, 2]\n",
    "    },\n",
    "    \"LinearSVC\": {\n",
    "        'clf__C': [0.1, 1, 10, 100],\n",
    "        'w2v__vector_size': [100, 200],\n",
    "        'w2v__window': [5, 10],\n",
    "        'w2v__min_count': [1, 2]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        'clf__n_estimators': [50, 100, 150],\n",
    "        'clf__max_depth': [None, 50, 100],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'w2v__vector_size': [100, 200],\n",
    "        'w2v__window': [5, 10],\n",
    "        'w2v__min_count': [1, 2]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for all models\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('w2v', Word2VecVectorizer()),  # Replace Tfidf with Word2VecVectorizer\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_distributions[model_name], \n",
    "                                n_iter=50, verbose=2, n_jobs=-1)  # Adjust n_iter as needed\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "\n",
    "    print(f\"Performance of {model_name} after Randomized Search\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: question\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Correct PR type: bug\n",
      "Predicted PR type: bug\n",
      "Accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Replace 'your-api-key' with your actual OpenAI API key\n",
    "openai.api_key = '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' ## ISAC'S KEY\n",
    "\n",
    "def query_chatgpt(prompt, model=\"gpt-4\", max_tokens=100):\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT-4 with a given prompt.\n",
    "    \n",
    "    :param prompt: Prompt string to send to ChatGPT-4\n",
    "    :param model: The model to use, default is ChatGPT-4\n",
    "    :param max_tokens: Maximum number of tokens to generate\n",
    "    :return: Response from ChatGPT-4\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are the best transformer for language classification.\"}, {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "train_data = pd.read_csv('./data/issues_train.csv')\n",
    "count = 0\n",
    "\n",
    "iterations = 20\n",
    "\n",
    "for i in range(iterations):\n",
    "    correctLabel = train_data.iloc[i]['label']\n",
    "    description = train_data.iloc[i]['title'] + train_data.iloc[i]['body']\n",
    "    print(f\"Correct PR type: {correctLabel}\")\n",
    "    # Example usage\n",
    "    prompt = f\"I will provide you with the description of a pull request on github. This description is composed by title and body of the pull request. Based on that description, I want you to answer me with just 1 word, this word can be either feature, bug or question. You will tell me whether the description given to you is a feature, a bug or a question. The definitions of those words is as follows: feature - new functionalty being added to a codebase, bug - a problem in existing code, question - an inquiry (anything that is not a feature or a bug). See pull request description below \\n {description}\"\n",
    "    response = query_chatgpt(prompt)\n",
    "    print(f\"Predicted PR type: {response}\")\n",
    "    if str(response) == correctLabel:\n",
    "        count += 1\n",
    "accuracy = count/iterations\n",
    "print(f\"Accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
